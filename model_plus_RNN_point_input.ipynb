{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\envs\\project2\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0=torch.zeros(self.n_layers, x.size(0), self.hidden_size).to('cuda')\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out=out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out \n",
    "network=RNN(2,32,15,2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "list_tar=[(1,2),(1,3),(1,4),(1,5),(1,6),(2,3),(2,4),(2,5),(2,6),(3,4),(3,5),(3,6),(4,5),(4,6),(5,6)]\n",
    "enu=enumerate(list_tar)\n",
    "encode_dict={tar:idx for idx,tar in enu}\n",
    "encode_dict[-1]=-1\n",
    "def encode(inp):\n",
    "    return encode_dict[int(inp)]\n",
    "def encode_list(inp_list):\n",
    "    return [encode_dict[tar] for tar in inp_list]\n",
    "enu=enumerate(list_tar)\n",
    "decode_dict={idx:tar for idx,tar in enu}\n",
    "decode_dict[-1]=-1\n",
    "def decode(inp):\n",
    "    return decode_dict[inp]\n",
    "enu=enumerate(list_tar)\n",
    "decode_dict_zero={idx:tar for idx,tar in enu}\n",
    "decode_dict_zero[-1]=(0,0)\n",
    "def decode_zero(inp):\n",
    "    return decode_dict_zero[inp]\n",
    "def decode_list(inp):\n",
    "    return [decode_dict[tar] for tar in inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "data=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    data['input_'+str(i)]=data['input_'+str(i)].apply(decode_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         self.dataframe = dataframe.iloc[:,1:]\n",
    "#         self.feature = torch.tensor(self.dataframe.iloc[:,15:].values.tolist(), dtype=torch.float32)\n",
    "#         self.label = torch.tensor(self.dataframe.iloc[:,:15].values.tolist(), dtype=torch.float32)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         feature = self.feature[index]\n",
    "#         label = self.label[index]\n",
    "#         return feature, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.label)\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe.iloc[:,1:]\n",
    "        self.feature = torch.tensor(self.dataframe.iloc[:,1:].values.tolist(), dtype=torch.float32)\n",
    "        self.label = F.one_hot(torch.tensor(self.dataframe.iloc[:,0].to_numpy())).to(torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.feature[index]\n",
    "        label = self.label[index]\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dataset = CustomDataset(data)\n",
    "train_set, val_set = torch.utils.data.random_split(pytorch_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_load = torch.utils.data.DataLoader(\n",
    "    val_set\n",
    "    , batch_size=len(val_set))\n",
    "x_val, y_val = next(iter(val_load))\n",
    "x_val = x_val.cuda()\n",
    "y_val = y_val.cuda()\n",
    "train_load = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    , batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=SummaryWriter('runs/RNN_point_input')\n",
    "inputs, labels = next(iter(train_load))\n",
    "writer.add_graph(network, inputs.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax().eq(labels).sum().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_accuracy():\n",
    "    preds = network(x_val)\n",
    "    return get_num_correct(preds, y_val) / y_val.shape[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loss(x_val):\n",
    "    network.eval()\n",
    "    x_val=x_val.reshape(x_val.shape[0],x_val.shape[1],2)\n",
    "    preds = network(x_val)\n",
    "    network.train()\n",
    "    return F.cross_entropy(preds, y_val)\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    network.train()\n",
    "    total_loss = 0\n",
    "    for i in train_load:\n",
    "        network.train()\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = i\n",
    "        features = features.reshape(features.shape[0],features.shape[1],2).to('cuda') # reshape and move to cuda\n",
    "        targets = targets.to('cuda')\n",
    "        preds = network(features)\n",
    "        loss = F.cross_entropy(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*features.shape[0]\n",
    "    scheduler.step(val_loss(x_val))\n",
    "    return f\"loss: {total_loss / len(train_set)}\\n val_loss: {val_loss(x_val)}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 2.7110855884552003\n",
      " val_loss: 2.7094645500183105\n",
      "\n",
      "epoch: 2 loss: 2.708787317276001\n",
      " val_loss: 2.7096307277679443\n",
      "\n",
      "epoch: 3 loss: 2.7083255577087404\n",
      " val_loss: 2.7097327709198\n",
      "\n",
      "epoch: 4 loss: 2.7080508880615235\n",
      " val_loss: 2.7097702026367188\n",
      "\n",
      "epoch: 5 loss: 2.707865287780762\n",
      " val_loss: 2.709773063659668\n",
      "\n",
      "epoch: 6 loss: 2.707738473892212\n",
      " val_loss: 2.70975399017334\n",
      "\n",
      "epoch: 7 loss: 2.707650775909424\n",
      " val_loss: 2.7097198963165283\n",
      "\n",
      "epoch: 8 loss: 2.7075884456634522\n",
      " val_loss: 2.709676742553711\n",
      "\n",
      "epoch: 9 loss: 2.7075419330596926\n",
      " val_loss: 2.709630012512207\n",
      "\n",
      "epoch: 10 loss: 2.707504508972168\n",
      " val_loss: 2.709584951400757\n",
      "\n",
      "epoch: 11 loss: 2.707471544265747\n",
      " val_loss: 2.709545135498047\n",
      "\n",
      "epoch: 12 loss: 2.7074403553009034\n",
      " val_loss: 2.709512233734131\n",
      "\n",
      "epoch: 13 loss: 2.70651243019104\n",
      " val_loss: 2.709618330001831\n",
      "\n",
      "epoch: 14 loss: 2.706437490463257\n",
      " val_loss: 2.7097489833831787\n",
      "\n",
      "epoch: 15 loss: 2.7063681507110595\n",
      " val_loss: 2.709888219833374\n",
      "\n",
      "epoch: 16 loss: 2.7063077602386474\n",
      " val_loss: 2.710026264190674\n",
      "\n",
      "epoch: 17 loss: 2.7062573490142823\n",
      " val_loss: 2.7101542949676514\n",
      "\n",
      "epoch: 18 loss: 2.706216403961182\n",
      " val_loss: 2.71026611328125\n",
      "\n",
      "epoch: 19 loss: 2.7061834831237794\n",
      " val_loss: 2.7103590965270996\n",
      "\n",
      "epoch: 20 loss: 2.7061568965911866\n",
      " val_loss: 2.710434675216675\n",
      "\n",
      "epoch: 21 loss: 2.7061350994110107\n",
      " val_loss: 2.7104952335357666\n",
      "\n",
      "epoch: 22 loss: 2.7061168365478516\n",
      " val_loss: 2.7105443477630615\n",
      "\n",
      "epoch: 23 loss: 2.7061012516021727\n",
      " val_loss: 2.7105846405029297\n",
      "\n",
      "epoch: 24 loss: 2.7059473781585694\n",
      " val_loss: 2.7105939388275146\n",
      "\n",
      "epoch: 25 loss: 2.705944881439209\n",
      " val_loss: 2.7106032371520996\n",
      "\n",
      "epoch: 26 loss: 2.7059424571990967\n",
      " val_loss: 2.7106125354766846\n",
      "\n",
      "epoch: 27 loss: 2.7059400672912597\n",
      " val_loss: 2.7106215953826904\n",
      "\n",
      "epoch: 28 loss: 2.705937774658203\n",
      " val_loss: 2.710629940032959\n",
      "\n",
      "epoch: 29 loss: 2.7059355087280275\n",
      " val_loss: 2.7106387615203857\n",
      "\n",
      "epoch: 30 loss: 2.7059332733154298\n",
      " val_loss: 2.710646867752075\n",
      "\n",
      "epoch: 31 loss: 2.7059311237335204\n",
      " val_loss: 2.7106549739837646\n",
      "\n",
      "epoch: 32 loss: 2.7059290294647216\n",
      " val_loss: 2.710662841796875\n",
      "\n",
      "epoch: 33 loss: 2.7059269733428954\n",
      " val_loss: 2.710670232772827\n",
      "\n",
      "epoch: 34 loss: 2.7059249744415284\n",
      " val_loss: 2.7106776237487793\n",
      "\n",
      "epoch: 35 loss: 2.705909004211426\n",
      " val_loss: 2.7106785774230957\n",
      "\n",
      "epoch: 36 loss: 2.70590877532959\n",
      " val_loss: 2.710679292678833\n",
      "\n",
      "epoch: 37 loss: 2.705908573150635\n",
      " val_loss: 2.7106802463531494\n",
      "\n",
      "epoch: 38 loss: 2.7059083557128907\n",
      " val_loss: 2.7106807231903076\n",
      "\n",
      "epoch: 39 loss: 2.7059081687927247\n",
      " val_loss: 2.710681676864624\n",
      "\n",
      "epoch: 40 loss: 2.705907960891724\n",
      " val_loss: 2.7106823921203613\n",
      "\n",
      "epoch: 41 loss: 2.705907751083374\n",
      " val_loss: 2.7106833457946777\n",
      "\n",
      "epoch: 42 loss: 2.705907541275024\n",
      " val_loss: 2.710684061050415\n",
      "\n",
      "epoch: 43 loss: 2.705907331466675\n",
      " val_loss: 2.7106847763061523\n",
      "\n",
      "epoch: 44 loss: 2.70590712928772\n",
      " val_loss: 2.7106857299804688\n",
      "\n",
      "epoch: 45 loss: 2.7059069499969484\n",
      " val_loss: 2.710686206817627\n",
      "\n",
      "epoch: 46 loss: 2.705905366897583\n",
      " val_loss: 2.710686445236206\n",
      "\n",
      "epoch: 47 loss: 2.705905330657959\n",
      " val_loss: 2.710686445236206\n",
      "\n",
      "epoch: 48 loss: 2.705905321121216\n",
      " val_loss: 2.710686683654785\n",
      "\n",
      "epoch: 49 loss: 2.7059052791595457\n",
      " val_loss: 2.710686683654785\n",
      "\n",
      "epoch: 50 loss: 2.7059052734375\n",
      " val_loss: 2.7106869220733643\n",
      "\n",
      "epoch: 51 loss: 2.7059052410125735\n",
      " val_loss: 2.7106869220733643\n",
      "\n",
      "epoch: 52 loss: 2.7059052066802978\n",
      " val_loss: 2.7106869220733643\n",
      "\n",
      "epoch: 53 loss: 2.7059052085876463\n",
      " val_loss: 2.7106869220733643\n",
      "\n",
      "epoch: 54 loss: 2.7059052047729493\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 55 loss: 2.705905153274536\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 56 loss: 2.7059051380157473\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 57 loss: 2.7059049682617187\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 58 loss: 2.70590496635437\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 59 loss: 2.705904970169067\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 60 loss: 2.7059049739837646\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 61 loss: 2.705904983520508\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 62 loss: 2.7059049587249757\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 63 loss: 2.705904970169067\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 64 loss: 2.7059049568176268\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 65 loss: 2.705904960632324\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 66 loss: 2.7059049549102783\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 67 loss: 2.705904947280884\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 68 loss: 2.7059049510955813\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 69 loss: 2.7059049568176268\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 70 loss: 2.7059049530029298\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 71 loss: 2.7059049396514894\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 72 loss: 2.7059049396514894\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 73 loss: 2.705904935836792\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 74 loss: 2.705904932022095\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 75 loss: 2.705904930114746\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 76 loss: 2.7059049243927\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 77 loss: 2.7059049282073975\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 78 loss: 2.705904926300049\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 79 loss: 2.7059049339294434\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 80 loss: 2.705904947280884\n",
      " val_loss: 2.7106871604919434\n",
      "\n",
      "epoch: 81 loss: 2.7059049243927\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 82 loss: 2.705904926300049\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 83 loss: 2.7059049224853515\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 84 loss: 2.7059049377441404\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 85 loss: 2.7059049396514894\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 86 loss: 2.7059049167633056\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 87 loss: 2.7059049377441404\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 88 loss: 2.7059049224853515\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 89 loss: 2.7059049282073975\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 90 loss: 2.705904914855957\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 91 loss: 2.705904909133911\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 92 loss: 2.705904899597168\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 93 loss: 2.7059049110412596\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 94 loss: 2.7059049110412596\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 95 loss: 2.7059049243927\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 96 loss: 2.705904920578003\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 97 loss: 2.705904909133911\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 98 loss: 2.7059049015045167\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 99 loss: 2.7059048976898192\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 100 loss: 2.7059048881530763\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 101 loss: 2.705904903411865\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 102 loss: 2.7059048976898192\n",
      " val_loss: 2.7106873989105225\n",
      "\n",
      "epoch: 103 loss: 2.705904899597168\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 104 loss: 2.705904909133911\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 105 loss: 2.7059048976898192\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 106 loss: 2.705904899597168\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 107 loss: 2.705904880523682\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 108 loss: 2.7059048938751222\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 109 loss: 2.705904880523682\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 110 loss: 2.705904867172241\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 111 loss: 2.705904884338379\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 112 loss: 2.7059048919677733\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 113 loss: 2.705904867172241\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 114 loss: 2.70590486907959\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 115 loss: 2.7059048595428465\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 116 loss: 2.705904880523682\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 117 loss: 2.705904867172241\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 118 loss: 2.705904863357544\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 119 loss: 2.705904872894287\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 120 loss: 2.705904851913452\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 121 loss: 2.705904846191406\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 122 loss: 2.7059048595428465\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 123 loss: 2.705904874801636\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 124 loss: 2.705904851913452\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 125 loss: 2.7059048614501955\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 126 loss: 2.705904848098755\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 127 loss: 2.7059048500061036\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 128 loss: 2.705904846191406\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 129 loss: 2.7059048347473142\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 130 loss: 2.705904836654663\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 131 loss: 2.7059048385620117\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 132 loss: 2.705904842376709\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 133 loss: 2.705904836654663\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 134 loss: 2.7059048309326172\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 135 loss: 2.7059048099517824\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 136 loss: 2.705904823303223\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 137 loss: 2.705904806137085\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 138 loss: 2.7059048137664794\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 139 loss: 2.7059048042297364\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 140 loss: 2.705904800415039\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 141 loss: 2.705904821395874\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 142 loss: 2.7059048042297364\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 143 loss: 2.705904823303223\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 144 loss: 2.7059047927856446\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 145 loss: 2.7059048080444335\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 146 loss: 2.7059048080444335\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 147 loss: 2.7059048137664794\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 148 loss: 2.705904794692993\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 149 loss: 2.7059047985076905\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 150 loss: 2.705904800415039\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 151 loss: 2.705904790878296\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 152 loss: 2.705904800415039\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 153 loss: 2.705904794692993\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 154 loss: 2.7059047756195067\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 155 loss: 2.7059047756195067\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 156 loss: 2.705904760360718\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 157 loss: 2.7059047698974608\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 158 loss: 2.7059047679901123\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 159 loss: 2.7059047927856446\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 160 loss: 2.70590478515625\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 161 loss: 2.705904773712158\n",
      " val_loss: 2.7106876373291016\n",
      "\n",
      "epoch: 162 loss: 2.7059047756195067\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 163 loss: 2.705904760360718\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 164 loss: 2.7059047660827638\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 165 loss: 2.7059047565460204\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 166 loss: 2.705904760360718\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 167 loss: 2.705904773712158\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 168 loss: 2.7059047641754153\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 169 loss: 2.7059047679901123\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 170 loss: 2.7059047622680663\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 171 loss: 2.7059047660827638\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 172 loss: 2.7059047775268557\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 173 loss: 2.7059047660827638\n",
      " val_loss: 2.7106878757476807\n",
      "\n",
      "epoch: 174 loss: 2.7059047565460204\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 175 loss: 2.705904754638672\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 176 loss: 2.7059047584533693\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 177 loss: 2.705904748916626\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 178 loss: 2.705904760360718\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 179 loss: 2.7059047527313234\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 180 loss: 2.705904731750488\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 181 loss: 2.7059047260284426\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 182 loss: 2.705904727935791\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 183 loss: 2.7059047412872315\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 184 loss: 2.705904733657837\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 185 loss: 2.705904739379883\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 186 loss: 2.705904737472534\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 187 loss: 2.705904727935791\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 188 loss: 2.7059047622680663\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 189 loss: 2.7059047412872315\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 190 loss: 2.7059047355651855\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 191 loss: 2.705904727935791\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 192 loss: 2.7059047183990477\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 193 loss: 2.7059047183990477\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 194 loss: 2.7059047145843507\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 195 loss: 2.7059047203063966\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 196 loss: 2.7059047145843507\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 197 loss: 2.7059046955108643\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 198 loss: 2.7059046955108643\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 199 loss: 2.7059047012329103\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 200 loss: 2.7059047031402588\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 201 loss: 2.705904710769653\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 202 loss: 2.7059047088623047\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 203 loss: 2.7059046955108643\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 204 loss: 2.70590468788147\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 205 loss: 2.7059046993255613\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 206 loss: 2.7059046955108643\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 207 loss: 2.705904691696167\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 208 loss: 2.705904691696167\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 209 loss: 2.7059046745300295\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 210 loss: 2.7059046669006346\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 211 loss: 2.705904680252075\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 212 loss: 2.705904680252075\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 213 loss: 2.7059046840667724\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 214 loss: 2.705904676437378\n",
      " val_loss: 2.710688352584839\n",
      "\n",
      "epoch: 215 loss: 2.705904661178589\n",
      " val_loss: 2.7106881141662598\n",
      "\n",
      "epoch: 216 loss: 2.7059046745300295\n",
      " val_loss: 2.710688352584839\n",
      "\n",
      "epoch: 217 loss: 2.7059046630859376\n",
      " val_loss: 2.710688352584839\n",
      "\n",
      "epoch: 218 loss: 2.705904676437378\n",
      " val_loss: 2.710688352584839\n",
      "\n",
      "epoch: 219 loss: 2.7059046459197997\n",
      " val_loss: 2.710688352584839\n",
      "\n",
      "epoch: 220 loss: 2.705904661178589\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 221 loss: 2.7059046573638916\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 222 loss: 2.705904661178589\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 223 loss: 2.705904655456543\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 224 loss: 2.7059046745300295\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 225 loss: 2.7059046630859376\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 226 loss: 2.7059046573638916\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 227 loss: 2.705904653549194\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 228 loss: 2.7059046516418457\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 229 loss: 2.705904653549194\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 230 loss: 2.7059046421051027\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 231 loss: 2.7059046382904053\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 232 loss: 2.7059046268463134\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 233 loss: 2.7059046325683593\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 234 loss: 2.705904624938965\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 235 loss: 2.705904613494873\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 236 loss: 2.7059046173095704\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 237 loss: 2.7059046173095704\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 238 loss: 2.705904634475708\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 239 loss: 2.705904619216919\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 240 loss: 2.7059046058654785\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 241 loss: 2.7059046115875245\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 242 loss: 2.7059046058654785\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 243 loss: 2.7059046154022215\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 244 loss: 2.705904619216919\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 245 loss: 2.7059046115875245\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 246 loss: 2.705904609680176\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 247 loss: 2.705904624938965\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 248 loss: 2.705904607772827\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 249 loss: 2.70590460395813\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 250 loss: 2.7059046154022215\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 251 loss: 2.7059046058654785\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 252 loss: 2.705904624938965\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 253 loss: 2.7059045886993407\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 254 loss: 2.705904596328735\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 255 loss: 2.705904602050781\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 256 loss: 2.7059045944213866\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 257 loss: 2.705904586791992\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 258 loss: 2.7059045848846437\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 259 loss: 2.7059045848846437\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 260 loss: 2.705904609680176\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 261 loss: 2.7059045906066896\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 262 loss: 2.7059045906066896\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 263 loss: 2.7059045848846437\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 264 loss: 2.7059045810699462\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 265 loss: 2.705904573440552\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 266 loss: 2.7059045658111573\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 267 loss: 2.7059045810699462\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 268 loss: 2.7059045772552492\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 269 loss: 2.7059045715332033\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 270 loss: 2.7059045715332033\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 271 loss: 2.7059045696258544\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 272 loss: 2.705904567718506\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 273 loss: 2.7059045600891114\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 274 loss: 2.7059045543670655\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 275 loss: 2.705904550552368\n",
      " val_loss: 2.710688591003418\n",
      "\n",
      "epoch: 276 loss: 2.705904544830322\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 277 loss: 2.705904544830322\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 278 loss: 2.705904535293579\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 279 loss: 2.7059045429229736\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 280 loss: 2.705904535293579\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 281 loss: 2.705904541015625\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 282 loss: 2.705904541015625\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 283 loss: 2.705904535293579\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 284 loss: 2.705904552459717\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 285 loss: 2.705904541015625\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 286 loss: 2.705904541015625\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 287 loss: 2.705904541015625\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 288 loss: 2.705904535293579\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 289 loss: 2.705904544830322\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 290 loss: 2.7059045314788817\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 291 loss: 2.705904544830322\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 292 loss: 2.705904525756836\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 293 loss: 2.705904525756836\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 294 loss: 2.7059045181274413\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 295 loss: 2.70590450668335\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 296 loss: 2.705904499053955\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 297 loss: 2.7059045162200928\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 298 loss: 2.7059045143127443\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 299 loss: 2.7059045143127443\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 300 loss: 2.7059045162200928\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 301 loss: 2.70590450668335\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 302 loss: 2.705904489517212\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 303 loss: 2.705904495239258\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 304 loss: 2.7059044914245605\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 305 loss: 2.7059044971466064\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 306 loss: 2.705904495239258\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 307 loss: 2.705904483795166\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 308 loss: 2.7059044971466064\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 309 loss: 2.7059045028686524\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 310 loss: 2.705904493331909\n",
      " val_loss: 2.710688829421997\n",
      "\n",
      "epoch: 311 loss: 2.7059044971466064\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 312 loss: 2.7059044876098635\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 313 loss: 2.7059044761657716\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 314 loss: 2.70590447807312\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 315 loss: 2.7059044799804686\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 316 loss: 2.705904510498047\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 317 loss: 2.7059044799804686\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 318 loss: 2.705904493331909\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 319 loss: 2.7059044799804686\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 320 loss: 2.705904474258423\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 321 loss: 2.705904466629028\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 322 loss: 2.705904474258423\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 323 loss: 2.705904466629028\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 324 loss: 2.705904474258423\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 325 loss: 2.7059044609069822\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 326 loss: 2.705904457092285\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 327 loss: 2.705904438018799\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 328 loss: 2.705904443740845\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 329 loss: 2.705904441833496\n",
      " val_loss: 2.710689067840576\n",
      "\n",
      "epoch: 330 loss: 2.7059044513702393\n",
      " val_loss: 2.710689067840576\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\onedrive\\OneDrive - connect.hku.hk\\桌面\\CCST9017 008\\model_plus_RNN_point_input.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     m \u001b[39m=\u001b[39m train_one_epoch()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch:\u001b[39m\u001b[39m'\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, m)\n",
      "\u001b[1;32me:\\onedrive\\OneDrive - connect.hku.hk\\桌面\\CCST9017 008\\model_plus_RNN_point_input.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m features, targets \u001b[39m=\u001b[39m i\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39;49mreshape(features\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],features\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# reshape and move to cuda\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN_point_input.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m preds \u001b[39m=\u001b[39m network(features)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    m = train_one_epoch()\n",
    "    print('epoch:', i + 1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_old_network(inp:torch.tensor):\n",
    "    new_inp=[decode_zero(i[0]) for i in inp[0].tolist()]\n",
    "    return network(torch.tensor(new_inp,dtype=torch.float32).reshape(1,15,2).to('cuda'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict':network.state_dict(),'optimizer_state_dict':optimizer.state_dict()},f'./models/model_RNN_two_point.pth')\n",
    "from itertools import combinations\n",
    "network.eval()\n",
    "list_tar=[(1,2),(1,3),(1,4),(1,5),(1,6),(2,3),(2,4),(2,5),(2,6),(3,4),(3,5),(3,6),(4,5),(4,6),(5,6)]\n",
    "enu=enumerate(list_tar)\n",
    "encode_dict={tar:idx for idx,tar in enu}\n",
    "encode_dict[-1]=-1\n",
    "def encode(inp):\n",
    "    return encode_dict[inp]\n",
    "def encode_list(inp_list):\n",
    "    return [encode_dict[tar] for tar in inp_list]\n",
    "enu=enumerate(list_tar)\n",
    "decode_dict={idx:tar for idx,tar in enu}\n",
    "decode_dict[-1]=-1\n",
    "def decode(inp):\n",
    "    return decode_dict[inp]\n",
    "def decode_list(inp):\n",
    "    return [decode_dict[tar] for tar in inp]\n",
    "def form_tri(lst: list,inp: set):\n",
    "    line_list=[x for x in lst+[inp] if x != -1]\n",
    "    combs = list(combinations(line_list, 3))\n",
    "    for comb in combs:\n",
    "        (a,b),(c,d),(e,f)=comb\n",
    "        if len(set([a,b,c,d,e,f]))==3:\n",
    "            return True\n",
    "    return False\n",
    "def make_move(inp: list): # inptut is should be encoded.\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    network.eval()\n",
    "    out=softmax(to_old_network(torch.tensor([inp],dtype=torch.float32).reshape(1,15,1).to('cuda'))).tolist()[0]\n",
    "    descending_indices = sorted(list(range(len(out))), key=lambda i: out[i],reverse=True)\n",
    "    for idx in descending_indices:\n",
    "        if (idx in inp) or form_tri(decode_list(inp[1::2]),decode(idx)):\n",
    "            continue\n",
    "        else:\n",
    "            return decode(idx)\n",
    "    for idx in descending_indices:\n",
    "        if (idx in inp):\n",
    "            continue\n",
    "        else:\n",
    "            return decode(idx)\n",
    "def make_move_no_assistance(inp: list):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    network.eval()\n",
    "    out=softmax(to_old_network(torch.tensor([inp],dtype=torch.float32).reshape(1,15,1).to('cuda'))).tolist()[0]\n",
    "    descending_indices = sorted(list(range(len(out))), key=lambda i: out[i],reverse=True)\n",
    "    for idx in descending_indices:\n",
    "        if (idx in inp):\n",
    "            continue\n",
    "        else:\n",
    "            return decode(idx)\n",
    "from random import choice\n",
    "def find_pos(inp_list):\n",
    "    for i in range(2,7):\n",
    "        lst=inp_list[:i+1]\n",
    "        combs = list(combinations(lst, 3))\n",
    "        combs=sorted(combs)\n",
    "        for comb in combs:\n",
    "            (a,b),(c,d),(e,f)=comb\n",
    "            if len(set([a,b,c,d,e,f]))==3:\n",
    "                return i\n",
    "    return 7\n",
    "# 0 stands for the the player one win, 1 stands for the player two win\n",
    "def determine_winner(inp_list):\n",
    "    pos_1=find_pos(inp_list[::2])\n",
    "    pos_2=find_pos(inp_list[1::2])\n",
    "    if pos_1<=pos_2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ai_play_against_random():\n",
    "    output=[-1 for _ in range(15)]\n",
    "    pos=0\n",
    "    pick=choice(list_tar)\n",
    "    output[pos]=pick\n",
    "    pos+=1\n",
    "    for i in range(7):\n",
    "        output[pos]=make_move_no_assistance(encode_list(output))\n",
    "        pos+=1\n",
    "        pick=choice([x for x in list_tar if x not in output])\n",
    "        output[pos]=pick\n",
    "        pos+=1\n",
    "    return determine_winner(decode_list(encode_list(output)))\n",
    "\n",
    "def ai_play_against_naive():\n",
    "    output=[-1 for _ in range(15)]\n",
    "    pos=0\n",
    "    pick=choice(list_tar)\n",
    "    output[pos]=pick\n",
    "    pos+=1\n",
    "    output[pos]=make_move(encode_list(output))\n",
    "    pos+=1\n",
    "    pick=choice([x for x in list_tar if x not in output])\n",
    "    output[pos]=pick\n",
    "    pos+=1\n",
    "    for i in range(6):\n",
    "        output[pos]=make_move(encode_list(output))\n",
    "        pos+=1\n",
    "        temp_list=[x for x in list_tar if x not in output and x != -1]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output[pos]=pick\n",
    "        pos+=1\n",
    "    return determine_winner(decode_list(encode_list(output)))\n",
    "\n",
    "def naive_against_radom():\n",
    "    output=[]\n",
    "    for i in range(5):\n",
    "        pick=choice(list_tar)\n",
    "        output.append(pick)\n",
    "    for i in range(5):\n",
    "        temp_list=[x for x in list_tar if x not in output]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[1::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output.append(pick)\n",
    "        while pick in output:\n",
    "            pick=choice(list_tar)\n",
    "        output.append(pick)\n",
    "    return determine_winner(output)\n",
    "\n",
    "def naive_against_naive():\n",
    "    output=[]\n",
    "    for i in range(4):\n",
    "        pick=choice([x for x in list_tar if x not in output])\n",
    "        output.append(pick)\n",
    "    for i in range(5):\n",
    "        temp_list=[x for x in list_tar if x not in output]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output.append(pick)\n",
    "        temp_list=[x for x in list_tar if x not in output]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[1::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output.append(pick)\n",
    "    pick=choice([x for x in list_tar if x not in output])\n",
    "    output.append(pick)\n",
    "    return determine_winner(output)\n",
    "\n",
    "def random_against_random():\n",
    "    output=[]\n",
    "    for i in range(15):\n",
    "        pick=choice([x for x in list_tar if x not in output])\n",
    "        output.append(pick)\n",
    "    return determine_winner(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.576\n"
     ]
    }
   ],
   "source": [
    "# network=torch.load('./models/model_64_36_original_plain.pth').to('cuda')\n",
    "network.eval()\n",
    "n=0\n",
    "for i in range(1000):\n",
    "    n+=ai_play_against_random()\n",
    "print(n/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RNN.forward() got an unexpected keyword argument 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\onedrive\\OneDrive - connect.hku.hk\\桌面\\CCST9017 008\\model_plus_RNN.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m softmax \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m=\u001b[39mnetwork(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m0\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m6\u001b[39;49m,\u001b[39m12\u001b[39;49m,\u001b[39m14\u001b[39;49m,\u001b[39m9\u001b[39;49m,\u001b[39m10\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]],dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/onedrive/OneDrive%20-%20connect.hku.hk/%E6%A1%8C%E9%9D%A2/CCST9017%20008/model_plus_RNN.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ascending_indices \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(out))), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m i: out[i],reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\project2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\project2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: RNN.forward() got an unexpected keyword argument 'input'"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "out=network(input=torch.tensor([[0,3,6,12,14,9,10,5,-1,-1,-1,-1,-1,-1,-1]],dtype=torch.float32).to('cuda')).tolist()[0]\n",
    "print(out)\n",
    "ascending_indices = sorted(list(range(len(out))), key=lambda i: out[i],reverse=True)\n",
    "ascending_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2020, -0.1966, -0.1875, -0.1714, -0.1749, -0.0718, -0.0715, -0.0306,\n",
       "          0.0674,  0.0718,  0.0226,  0.1946,  0.1817,  0.1759,  0.3197]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network(torch.tensor([[10,0,5,14,8,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]],dtype=torch.float32).to('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
