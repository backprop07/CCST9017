{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer=SummaryWriter('runs_res_large\\cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.3\n",
    "network=nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(225,64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(64,16),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,15)).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left=nn.Sequential(\n",
    "            nn.Conv2d(inchannel,outchannel,kernel_size=3,stride=stride,padding=1),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel,outchannel,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(outchannel))\n",
    "        self.right=nn.Conv2d(inchannel,outchannel,kernel_size=1,stride=stride)\n",
    "    def forward(self,x):\n",
    "        out=self.left(x)\n",
    "        out=out+self.right(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet16, self).__init__()\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),)\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(2048, 512)\n",
    "        self.fc1 = nn.Linear(512, 15)\n",
    "\n",
    "    def _make_layer(self, inchannel, outchannel, block_num, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(inchannel, outchannel, stride))\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock(outchannel, outchannel))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pre(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x=self.fc(x)\n",
    "        x=self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, hidden_size=32, num_layers=2):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = num_layers\n",
    "        self.lstm = nn.LSTM(15, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 15)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to('cuda')\n",
    "        x, _ = self.lstm(x, (h0, h0))  # Use (h0, h0) for both hx and cx\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_out=0\n",
    "class block_change(nn.Module):\n",
    "    def __init__(self,in_features,out_features):\n",
    "        super(block_change, self).__init__()\n",
    "        self.fc1=nn.Linear(in_features, out_features)\n",
    "        self.bn1=nn.BatchNorm1d(out_features)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.dropout1=nn.Dropout(drop_out)\n",
    "        self.fc2=nn.Linear(out_features, out_features)\n",
    "        self.bn2=nn.BatchNorm1d(out_features)\n",
    "        self.dropout2=nn.Dropout(drop_out)\n",
    "        self.shortcut=nn.Sequential(nn.Linear(in_features, out_features),nn.BatchNorm1d(out_features),nn.Dropout(drop_out))\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        temp=x.clone()\n",
    "        x=self.fc1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.dropout1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=x+self.shortcut(temp)\n",
    "        x=self.relu2(x)\n",
    "        return x\n",
    "\n",
    "Res_seq= nn.Sequential( nn.Flatten(),\n",
    "                        nn.Linear(225, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                      *[block_change(2**i,2**(i+1)) for i in range(8,10)],\n",
    "                      *[block_change(2**i,2**(i-1)) for i in range(12,4,-1)],\n",
    "                      nn.Linear(16, 15))\n",
    "\n",
    "Res_small=nn.Sequential(nn.Flatten(),\n",
    "                        nn.Linear(225, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        *[block_change(2**i,2**(i-1)) for i in range(8,4,-1)],\n",
    "                        nn.Linear(16, 15))\n",
    "                        \n",
    "Res_large= nn.Sequential( nn.Flatten(),\n",
    "                        nn.Linear(225, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                      *[block_change(256,256) for i in range(8)],\n",
    "                      *[block_change(2**i,2**(i-1)) for i in range(8,4,-1)],\n",
    "                      nn.Linear(16, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Res_large.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count the number of parameters\n",
    "num_params = count_parameters(network)\n",
    "\n",
    "print(\"Number of parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "data=pd.read_csv('train_one_hotted_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=data[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_inp(data:pd.DataFrame):\n",
    "    list_of_in=data.values.tolist()\n",
    "    list_of_out=[]\n",
    "    for i in list_of_in:\n",
    "        temp=[[0]*15 for _ in range(15)]\n",
    "        for j in range(15):\n",
    "            if i[j]!=-1:\n",
    "                temp[j][int(i[j])]=1\n",
    "        list_of_out.append(temp)\n",
    "    return list_of_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe.iloc[:,1:]\n",
    "        self.feature = torch.tensor(one_hot_inp(data.iloc[:,15:]), dtype=torch.float32)\n",
    "        self.label = torch.tensor(self.dataframe.iloc[:,:15].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.feature[index]\n",
    "        label = self.label[index]\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         self.dataframe = dataframe.iloc[:,1:]\n",
    "#         self.feature = torch.tensor(self.dataframe.iloc[:,1:].to_numpy(), dtype=torch.float32)\n",
    "#         self.label = F.one_hot(torch.tensor(self.dataframe.iloc[:,0].to_numpy())).to(torch.float32)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         feature = self.feature[index]\n",
    "#         label = self.label[index]\n",
    "#         return feature, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dataset = CustomDataset(data)\n",
    "train_set, val_set = torch.utils.data.random_split(pytorch_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_load = torch.utils.data.DataLoader(\n",
    "    val_set\n",
    "    , batch_size=len(val_set))\n",
    "x_val, y_val = next(iter(val_load))\n",
    "x_val = x_val.cuda()\n",
    "y_val = y_val.cuda()\n",
    "train_load = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    , batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=5)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax().eq(labels).sum().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_accuracy():\n",
    "    preds = network(x_val)\n",
    "    return get_num_correct(preds, y_val) / y_val.shape[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loss():\n",
    "    network.eval()\n",
    "    preds = network(x_val)\n",
    "    network.train()\n",
    "    return F.cross_entropy(preds, y_val)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    network.train()\n",
    "    total_loss = 0\n",
    "    for i in train_load:\n",
    "        network.train()\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = i\n",
    "        features=features.to('cuda')\n",
    "        targets=targets.to('cuda')\n",
    "        preds = network(features)\n",
    "        loss = F.cross_entropy(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*features.shape[0]\n",
    "    scheduler.step(val_loss())\n",
    "    writer.add_scalar('training loss', total_loss / len(train_set), epoch)\n",
    "    val=float(val_loss())\n",
    "    writer.add_scalar('validation loss', val, epoch)\n",
    "    return f\"loss: {total_loss / len(train_set)}\\n val_loss: {val}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train()\n",
    "for i in range(500):\n",
    "    m = train_one_epoch(epoch=i)\n",
    "    print('epoch:', i + 1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "strtime=datetime.datetime.now().strftime(\"%y-%m-%d\")\n",
    "torch.save({\"model\":network,\"weight\":network.state_dict(),\"optimizer\":optimizer.state_dict()}, f'res_large_{strtime}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "network.eval()\n",
    "list_tar=[(1,2),(1,3),(1,4),(1,5),(1,6),(2,3),(2,4),(2,5),(2,6),(3,4),(3,5),(3,6),(4,5),(4,6),(5,6)]\n",
    "enu=enumerate(list_tar)\n",
    "encode_dict={tar:idx for idx,tar in enu}\n",
    "encode_dict[-1]=-1\n",
    "def one_hot_list_inp(inp_list):\n",
    "    temp=[[0]*15 for _ in range(15)]\n",
    "    for j in range(15):\n",
    "        if inp_list[j]!=-1:\n",
    "            temp[j][int(inp_list[j])]=1\n",
    "    return temp\n",
    "def encode(inp):\n",
    "    return encode_dict[inp]\n",
    "def encode_list(inp_list):\n",
    "    return [encode_dict[tar] for tar in inp_list]\n",
    "enu=enumerate(list_tar)\n",
    "decode_dict={idx:tar for idx,tar in enu}\n",
    "decode_dict[-1]=-1\n",
    "def decode(inp):\n",
    "    return decode_dict[inp]\n",
    "def decode_list(inp):\n",
    "    return [decode_dict[tar] for tar in inp]\n",
    "def form_tri(lst: list,inp: set):\n",
    "    line_list=[x for x in lst+[inp] if x != -1]\n",
    "    combs = list(combinations(line_list, 3))\n",
    "    for comb in combs:\n",
    "        (a,b),(c,d),(e,f)=comb\n",
    "        if len(set([a,b,c,d,e,f]))==3:\n",
    "            return True\n",
    "    return False\n",
    "def make_move(inp: list): # inptut should be encoded.\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    network.eval()\n",
    "    inp1=one_hot_list_inp(inp)\n",
    "    out=softmax(network(torch.tensor([inp1],dtype=torch.float32).to('cuda'))).tolist()[0]\n",
    "    descending_indices = sorted(list(range(len(out))), key=lambda i: out[i],reverse=True)\n",
    "    for idx in descending_indices:\n",
    "        if (idx in inp) or form_tri(decode_list(inp[1::2]),decode(idx)):\n",
    "            continue\n",
    "        else:\n",
    "            return decode(idx)\n",
    "    for idx in descending_indices:\n",
    "        if (idx in inp):\n",
    "            continue\n",
    "        else:\n",
    "            return decode(idx)\n",
    "    return -1\n",
    "\n",
    "def make_move_no_assistance(inp: list):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    network.eval()\n",
    "    inp1=one_hot_list_inp(inp)\n",
    "    out=softmax(network(torch.tensor([inp1],dtype=torch.float32).to('cuda'))).tolist()[0]\n",
    "    descending_indices = sorted(list(range(len(out))), key=lambda i: out[i],reverse=True)\n",
    "    for idx in descending_indices:\n",
    "        if (idx in inp):\n",
    "            continue\n",
    "        else:\n",
    "            return decode(idx)\n",
    "    return -1\n",
    "\n",
    "from random import choice\n",
    "def find_pos(inp_list):\n",
    "    for i in range(2,7):\n",
    "        lst=inp_list[:i+1]\n",
    "        combs = list(combinations(lst, 3))\n",
    "        combs=sorted(combs)\n",
    "        for comb in combs:\n",
    "            (a,b),(c,d),(e,f)=comb\n",
    "            if len(set([a,b,c,d,e,f]))==3:\n",
    "                return i\n",
    "    return 7\n",
    "# 0 stands for the the player one win, 1 stands for the player two win\n",
    "def determine_winner(inp_list):\n",
    "    pos_1=find_pos(inp_list[::2])\n",
    "    pos_2=find_pos(inp_list[1::2])\n",
    "    if pos_1<=pos_2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ai_play_against_random():\n",
    "    output=[-1 for _ in range(15)]\n",
    "    pos=0\n",
    "    pick=choice(list_tar)\n",
    "    output[pos]=pick\n",
    "    pos+=1\n",
    "    for _ in range(7):\n",
    "        output[pos]=make_move_no_assistance(encode_list(output))\n",
    "        pos+=1\n",
    "        pick=choice([x for x in list_tar if x not in output])\n",
    "        output[pos]=pick\n",
    "        pos+=1\n",
    "    return determine_winner(decode_list(encode_list(output)))\n",
    "\n",
    "def ai_play_against_naive():\n",
    "    output=[-1 for _ in range(15)]\n",
    "    pos=0\n",
    "    pick=choice(list_tar)\n",
    "    output[pos]=pick\n",
    "    pos+=1\n",
    "    output[pos]=make_move(encode_list(output))\n",
    "    pos+=1\n",
    "    pick=choice([x for x in list_tar if x not in output])\n",
    "    output[pos]=pick\n",
    "    pos+=1\n",
    "    for i in range(6):\n",
    "        output[pos]=make_move(encode_list(output))\n",
    "        pos+=1\n",
    "        temp_list=[x for x in list_tar if x not in output and x != -1]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output[pos]=pick\n",
    "        pos+=1\n",
    "    return determine_winner(decode_list(encode_list(output)))\n",
    "\n",
    "def naive_against_radom():\n",
    "    output=[]\n",
    "    for i in range(5):\n",
    "        pick=choice(list_tar)\n",
    "        output.append(pick)\n",
    "    for i in range(5):\n",
    "        temp_list=[x for x in list_tar if x not in output]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[1::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output.append(pick)\n",
    "        while pick in output:\n",
    "            pick=choice(list_tar)\n",
    "        output.append(pick)\n",
    "    return determine_winner(output)\n",
    "\n",
    "def naive_against_naive():\n",
    "    output=[]\n",
    "    for i in range(4):\n",
    "        pick=choice([x for x in list_tar if x not in output])\n",
    "        output.append(pick)\n",
    "    for i in range(5):\n",
    "        temp_list=[x for x in list_tar if x not in output]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output.append(pick)\n",
    "        temp_list=[x for x in list_tar if x not in output]\n",
    "        temp_list1=temp_list.copy()\n",
    "        for j in range(len(temp_list)):\n",
    "            if form_tri(output[1::2],temp_list[j]):\n",
    "                temp_list1.remove(temp_list[j])\n",
    "        if temp_list1==[]:\n",
    "            pick=choice([x for x in list_tar if x not in output])\n",
    "        else:\n",
    "            pick=choice(temp_list1)\n",
    "        output.append(pick)\n",
    "    pick=choice([x for x in list_tar if x not in output])\n",
    "    output.append(pick)\n",
    "    return determine_winner(output)\n",
    "\n",
    "def random_against_random():\n",
    "    output=[]\n",
    "    for i in range(15):\n",
    "        pick=choice([x for x in list_tar if x not in output])\n",
    "        output.append(pick)\n",
    "    return determine_winner(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network=torch.load('./models/model_64_36_original_plain.pth').to('cuda')\n",
    "network.eval()\n",
    "n=0\n",
    "for i in range(10000):\n",
    "    n+=ai_play_against_naive()\n",
    "print(n/10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
